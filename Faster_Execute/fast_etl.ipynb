{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio\n",
    "from fast_sql_queries import *\n",
    "import json\n",
    "import csv\n",
    "from sqlalchemy_schemadisplay import create_schema_graph\n",
    "from sqlalchemy import MetaData\n",
    "from monitor_script import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def iter_song_data(filepath):\n",
    "    \"\"\"\n",
    "    Iterator function for reading song JSON files in filepath with one row inside and yield data as\n",
    "    dictionary\n",
    "    \n",
    "    Args:\n",
    "        filepath (str) -> root file path contain all json file\n",
    "    \"\"\"\n",
    "    # get all files matching extension from directory\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        for f in files :\n",
    "            if 'checkpoint' not in f:\n",
    "                all_files.append(os.path.abspath(f))\n",
    "\n",
    "    # get total number of files found\n",
    "    num_files = len(all_files)\n",
    "    print('{} files found in {}'.format(num_files, filepath))\n",
    "    \n",
    "    for file in all_files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        yield data\n",
    "        \n",
    "def song_data_extract(iter_file):\n",
    "    \"\"\"\n",
    "    Function that proccess song file iterator and accumulate correct data from all the files into a list of tuples.\n",
    "    \n",
    "    Args:\n",
    "        iter_file (iterator) -> Iterator for song files\n",
    "    \n",
    "    Returns:\n",
    "        song_lst (list) --> List of tuples contain valid data songs\n",
    "        artist_lst (list) --> List of tupels contai valid data for artist\n",
    "    \"\"\"\n",
    "    song_columns = [\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"]\n",
    "    artist_columns = [\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"]\n",
    "    song_lst = []\n",
    "    artist_lst = []\n",
    "    \n",
    "    for file in iter_file:\n",
    "        song_lst.append(tuple(file[k] for k in song_columns))\n",
    "        artist_lst.append(tuple(file[k] for k in artist_columns))\n",
    "        \n",
    "    return song_lst,artist_lst\n",
    "\n",
    "\n",
    "def log_file_extract(filepath):\n",
    "    \"\"\"\n",
    "    Function to process log files and accumulate all file data into one pandas dataframe\n",
    "    \n",
    "    Args:\n",
    "        filepath (str) -> root file path contain all json file\n",
    "        \n",
    "    Return:\n",
    "        df (pandas.dataframe) --> Dataframe with all data from files \n",
    "    \"\"\"\n",
    "    # get all files matching extension from directory\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        for f in files :\n",
    "            if 'checkpoint' not in f:\n",
    "                all_files.append(os.path.abspath(f))\n",
    "\n",
    "    # get total number of files found\n",
    "    num_files = len(all_files)\n",
    "    print('{} files found in {}'.format(num_files, filepath))\n",
    "    # collecting all files into dataframe\n",
    "    df = pd.read_json(all_files[0], lines = True)\n",
    "    for file in all_files[1:]:\n",
    "        df = df.append(pd.read_json(file, lines = True))\n",
    "    return df[df['page'] =='NextSong'] # filter data having page = NextSong\n",
    "        \n",
    "        \n",
    "def log_data_extract(cur,df):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def extract_song_artists_id(cur,row):\n",
    "        cur.execute(song_select, (row['song'], row['artist'], row['length']))\n",
    "        results = cur.fetchone()\n",
    "\n",
    "        if results:\n",
    "            songid, artistid = results\n",
    "        else:\n",
    "            songid, artistid = None, None\n",
    "        \n",
    "        row['song_id'] = songid\n",
    "        row['artist_id'] = artistid\n",
    "        return row\n",
    "    \n",
    "    time_columns = [\"ts\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\"]\n",
    "    user_columns = [\"userId\", \"firstName\", \"lastName\", \"gender\", \"level\"]\n",
    "    songplay_columns = ['ts', 'userId', 'level', 'song_id', 'artist_id', 'sessionId', 'location', 'userAgent']\n",
    "    \n",
    "    #songplay data\n",
    "    df['song_id'] = 0\n",
    "    df['artist_id'] = 0\n",
    "    df.apply(lambda x:extract_song_artists_id(cur,x),axis=1)\n",
    "    df['ts'] = pd.to_datetime(df['ts'],unit='ms')       \n",
    "    t = df['ts']\n",
    "    time_data = [t.values,t.dt.hour,t.dt.day, t.dt.week, t.dt.month, t.dt.year, t.dt.dayofweek]\n",
    "    time_df = pd.DataFrame({time_columns[i] : time_data[i] for i in range(len(time_data))})\n",
    "    #print(df[user_columns].values[0])\n",
    "    user_df = df[user_columns]\n",
    "    #songplay data\n",
    "    \n",
    "    songplay_data = df[songplay_columns]\n",
    "    return time_df,user_df,songplay_data\n",
    "\n",
    "def insert_mogrify(cur,tuples, insert, values_statement, clause):\n",
    "    \"\"\"\n",
    "    Using cursor.mogrify() to build the bulk insert query\n",
    "    then cursor.execute() to execute the query\n",
    "    \"\"\"\n",
    "    values = [cur.mogrify(values_statement, tup).decode('utf8') for tup in tuples]\n",
    "    query  = insert + \",\".join(values) + clause\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        print(\"execute_mogrify() done\")\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cur.close()\n",
    "        print(1)\n",
    "        \n",
    "def sql_insert_execution(cur, df, table_name, clause='ON CONFLICT DO NOTHING'):\n",
    "    \"\"\"\n",
    "    sdfa\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "    CREATE TABLE {table_name}_h AS\n",
    "    TABLE {table_name};\n",
    "    \n",
    "    COPY {table_name}_h FROM STDIN With CSV;\n",
    "\n",
    "    INSERT INTO {table_name}\n",
    "    SELECT *\n",
    "    FROM {table_name}_h {clause};\n",
    "\n",
    "    DROP TABLE {table_name}_h;\n",
    "    \"\"\"\n",
    "    file = './{}.csv'.format(table_name)\n",
    "    ix = False\n",
    "    if table_name == 'songplays':\n",
    "        ix = True\n",
    "    if type(df) != type(pd.DataFrame()):\n",
    "        with open(file,'w') as out:\n",
    "            csv_out=csv.writer(out)\n",
    "            csv_out.writerows(df)\n",
    "    else:\n",
    "        df.to_csv(file, index=ix, header=False)\n",
    "    with open(file, 'r') as f:\n",
    "        cur.copy_expert(sql,f)\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=sparkifydb user=student password=student\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "iter_file = iter_song_data('data/song_data/')\n",
    "song_data, artist_data = song_data_extract(iter_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#insert_mogrify(cur, song_data, song_table_insert, song_values, song_clause)\n",
    "#insert_mogrify(cur, artist_data, artist_table_insert, artist_values, artist_clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sql_insert_execution(cur, song_data, 'songs', clause=song_clause)\n",
    "sql_insert_execution(cur, artist_data, 'artists', clause=artist_clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "log_file = log_file_extract('data/log_data')\n",
    "time_df,user_df,songplay_data = log_data_extract(cur,log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sql_insert_execution(cur, time_df,'time', time_clause)\n",
    "sql_insert_execution(cur, user_df,'users', user_clause)\n",
    "sql_insert_execution(cur, songplay_data,'songplays', songplay_clause)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
